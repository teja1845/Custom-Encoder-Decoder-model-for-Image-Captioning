{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 2 - Part B: Evaluating on Occluded Images\n",
    "\n",
    "This notebook covers Part B of the assignment. We will:\n",
    "1. Implement image occlusion by masking patches.\n",
    "2. Evaluate the pre-trained SmolVLM (zero-shot) and the custom-trained ViT-GPT2 model (from Part A) on occluded test images.\n",
    "3. Analyze the change in performance (BLEU, ROUGE-L, METEOR) due to occlusion.\n",
    "4. Save the generated captions along with occlusion levels for Part C.\n",
    "\n",
    "## B1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:34:46.395571Z",
     "iopub.status.busy": "2025-04-11T01:34:46.395248Z",
     "iopub.status.idle": "2025-04-11T01:34:46.401577Z",
     "shell.execute_reply": "2025-04-11T01:34:46.400840Z",
     "shell.execute_reply.started": "2025-04-11T01:34:46.395541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.1 Downloading our trained model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:35:58.844167Z",
     "iopub.status.busy": "2025-04-11T01:35:58.843888Z",
     "iopub.status.idle": "2025-04-11T01:36:14.911789Z",
     "shell.execute_reply": "2025-04-11T01:36:14.911054Z",
     "shell.execute_reply.started": "2025-04-11T01:35:58.844147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1IS_jKcD0ginPrwo-OY14DutAsRF7cuMt\n",
      "From (redirected): https://drive.google.com/uc?id=1IS_jKcD0ginPrwo-OY14DutAsRF7cuMt&confirm=t&uuid=5ee4db82-b619-40a9-bd6d-b723a8b3ec84\n",
      "To: /kaggle/working/vit_gpt2_caption_model.pth\n",
      "100%|████████████████████████████████████████| 700M/700M [00:08<00:00, 84.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gdown\n",
    "\n",
    "# model stored in gdrive zip file\n",
    "file_id = \"1IS_jKcD0ginPrwo-OY14DutAsRF7cuMt\"\n",
    "!gdown {file_id} --output vit_gpt2_caption_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:36:14.913525Z",
     "iopub.status.busy": "2025-04-11T01:36:14.913246Z",
     "iopub.status.idle": "2025-04-11T01:36:26.333595Z",
     "shell.execute_reply": "2025-04-11T01:36:26.332753Z",
     "shell.execute_reply.started": "2025-04-11T01:36:14.913502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-4zt018qT1M85m1X0v95C9-a6_6YelIQ\n",
      "From (redirected): https://drive.google.com/uc?id=1-4zt018qT1M85m1X0v95C9-a6_6YelIQ&confirm=t&uuid=2cfdb06b-a0b7-41e6-b178-59a2fb173ecf\n",
      "To: /kaggle/working/custom_captions_dataset.zip\n",
      "100%|████████████████████████████████████████| 288M/288M [00:04<00:00, 71.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gdown\n",
    "\n",
    "# dataset stored in gdrive zip file\n",
    "file_id = \"1-4zt018qT1M85m1X0v95C9-a6_6YelIQ\"\n",
    "!gdown {file_id} --output custom_captions_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:36:26.334814Z",
     "iopub.status.busy": "2025-04-11T01:36:26.334551Z",
     "iopub.status.idle": "2025-04-11T01:36:28.952691Z",
     "shell.execute_reply": "2025-04-11T01:36:28.951787Z",
     "shell.execute_reply.started": "2025-04-11T01:36:26.334779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"custom_captions_dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"custom_captions_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Config and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:42:23.247234Z",
     "iopub.status.busy": "2025-04-11T06:42:23.246941Z",
     "iopub.status.idle": "2025-04-11T06:42:23.253211Z",
     "shell.execute_reply": "2025-04-11T06:42:23.252413Z",
     "shell.execute_reply.started": "2025-04-11T06:42:23.247214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Raw Results Dir: part_b_raw_results_csv\n",
      "Part C Output File: final_raw_results.csv\n"
     ]
    }
   ],
   "source": [
    "OCCLUSION_LEVELS_ARG = [0.1, 0.5, 0.8]\n",
    "PATCH_SIZE = 16\n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = \"custom_captions_dataset/custom_captions_dataset\"\n",
    "TEST_IMAGES_PATH = os.path.join(DATASET_PATH, \"test\")\n",
    "TEST_CSV_PATH = os.path.join(DATASET_PATH, \"test.csv\")\n",
    "CUSTOM_MODEL_PATH = \"vit_gpt2_caption_model.pth\"\n",
    "\n",
    "PART_C_DATA_OUTPUT_FILE = \"final_raw_results.csv\"\n",
    "RAW_RESULTS_DIR = \"part_b_raw_results_csv\"\n",
    "os.makedirs(RAW_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Raw Results Dir: {RAW_RESULTS_DIR}\")\n",
    "print(f\"Part C Output File: {PART_C_DATA_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Load Models and Processors/Tokenizers\n",
    "\n",
    "Load the SmolVLM and the custom trained ViT-GPT2 model from Part A.\n",
    "\n",
    "### B3.1 Load SmolVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:36:42.821998Z",
     "iopub.status.busy": "2025-04-11T01:36:42.821739Z",
     "iopub.status.idle": "2025-04-11T01:37:15.385185Z",
     "shell.execute_reply": "2025-04-11T01:37:15.384409Z",
     "shell.execute_reply.started": "2025-04-11T01:36:42.821979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156c4de84c4e4208b464860a033395fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11df395ed1454cd9872c9691ca3d4c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af961afdac347ce85310f59d5392a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5742f6d3f24239a72e09da96e9b635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e478f283bcc4d1e86ab0450c9620ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4d94e91d64359917ee0069c2c0670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ece2fa3e2f042138ae7ba8ca22c4699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a94c0ca7b2479fbeb5d0d6b03bc130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31334d5fd82c463e90b76f75836120ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7a6e8c23f344788268146093a055c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/7.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9bc790c7484a11b966a2243d309ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce4315aa35d4aaf8afc39f784e061b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolVLM loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "smolvlm_model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "try:\n",
    "    smolvlm_processor = AutoProcessor.from_pretrained(smolvlm_model_name)\n",
    "    smolvlm_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        smolvlm_model_name,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\" \n",
    "    ).to(DEVICE)\n",
    "    smolvlm_model.eval()\n",
    "    print(\"SmolVLM loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SmolVLM: {e}\")\n",
    "    smolvlm_model = None\n",
    "    smolvlm_processor = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3.2 Load Custom Trained Model\n",
    "\n",
    "We used the `ImageCaptionModel` class definition from Part A to load the state dict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:37:19.031877Z",
     "iopub.status.busy": "2025-04-11T01:37:19.031610Z",
     "iopub.status.idle": "2025-04-11T01:37:26.998860Z",
     "shell.execute_reply": "2025-04-11T01:37:26.998234Z",
     "shell.execute_reply.started": "2025-04-11T01:37:19.031859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830bdf40b6724cccb48142a920124272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c450895012431493001efa0499b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a8a716128d44d0b696d0d22ab583cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defe6e79e26246069036f4653c3e60a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ed8c1ce4ae46dd9bc860730dc9e3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce07182a98a4deaa6290d6dbb4a3afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360cba885f2e4729953cdc4a9fc365a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b8b7b06376424e8266c8d151037064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42842dcf1a3748f6b66a9d119fa205c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15380170aef848c58d5b27884821017b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trained model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Encoder-Decoder Model for Image Captioning using ViT as an encoder and GPT2 for decoder(same as from part A)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_model_name=\"WinKawaks/vit-small-patch16-224\", decoder_model_name=\"gpt2\", dropout_rate=0.1):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "            encoder_model_name, decoder_model_name\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "             self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "             self.model.config.decoder.pad_token_id = self.model.config.decoder.eos_token_id\n",
    "\n",
    "        self.model.config.decoder_start_token_id = self.tokenizer.bos_token_id\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained(encoder_model_name)\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, pixel_values, gen_kwargs=None):\n",
    "        if gen_kwargs is None:\n",
    "            gen_kwargs = {\"max_length\": 150, \"num_beams\": 4}\n",
    "\n",
    "        gen_kwargs['pad_token_id'] = self.model.config.pad_token_id\n",
    "        if self.model.config.decoder_start_token_id is None and hasattr(self.tokenizer, 'bos_token_id'):\n",
    "             gen_kwargs.setdefault('decoder_start_token_id', self.tokenizer.bos_token_id)\n",
    "        elif 'decoder_start_token_id' not in gen_kwargs:\n",
    "             gen_kwargs.setdefault('decoder_start_token_id', self.model.config.decoder_start_token_id)\n",
    "\n",
    "        return self.model.generate(pixel_values=pixel_values.to(self.model.device), **gen_kwargs) \n",
    "\n",
    "\n",
    "# load out custome model we fine tuned previously\n",
    "try:\n",
    "    custom_model = ImageCaptionModel(\n",
    "        encoder_model_name=\"WinKawaks/vit-small-patch16-224\",\n",
    "        decoder_model_name=\"gpt2\"\n",
    "    )\n",
    "    custom_model.load_state_dict(torch.load(CUSTOM_MODEL_PATH, map_location=DEVICE, weights_only=True))\n",
    "\n",
    "    custom_model.to(DEVICE)\n",
    "    custom_model.eval()\n",
    "    custom_tokenizer = custom_model.tokenizer\n",
    "    custom_image_processor = custom_model.image_processor\n",
    "    print(\"Custom trained model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Custom model state file not found at {CUSTOM_MODEL_PATH}. Cannot proceed with custom model evaluation.\")\n",
    "    custom_model = None\n",
    "    custom_tokenizer = None\n",
    "    custom_image_processor = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading custom model state: {e}\")\n",
    "    custom_model = None\n",
    "    custom_tokenizer = None\n",
    "    custom_image_processor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Image Occlusion Function\n",
    "\n",
    "Occludes images based on the mask percentage provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:37:33.282088Z",
     "iopub.status.busy": "2025-04-11T01:37:33.281817Z",
     "iopub.status.idle": "2025-04-11T01:37:33.288614Z",
     "shell.execute_reply": "2025-04-11T01:37:33.287852Z",
     "shell.execute_reply.started": "2025-04-11T01:37:33.282069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def occlude_image(image: np.array, mask_percentage: float, patch_size: int = 16) -> np.array:\n",
    "    \"\"\"\n",
    "    Applying patch-wise occlusion to an image. i/p and o/p are numpy arrays.\n",
    "    \"\"\"\n",
    "    if mask_percentage == 0:\n",
    "        return image.copy()\n",
    "\n",
    "    occluded_image = image.copy()\n",
    "    h, w, _ = occluded_image.shape\n",
    "\n",
    "    num_patches_h = math.ceil(h / patch_size)\n",
    "    num_patches_w = math.ceil(w / patch_size)\n",
    "    total_patches = num_patches_h * num_patches_w\n",
    "\n",
    "    num_patches_to_mask = int(total_patches * mask_percentage)\n",
    "\n",
    "    # all indices of patches\n",
    "    all_patch_indices = [(i, j) for i in range(num_patches_h) for j in range(num_patches_w)]\n",
    "\n",
    "    # randomly select patches to mask\n",
    "    indices_to_mask = random.sample(all_patch_indices, num_patches_to_mask)\n",
    "\n",
    "    for patch_r, patch_c in indices_to_mask:\n",
    "        r_start = patch_r * patch_size\n",
    "        r_end = min((patch_r + 1) * patch_size, h)\n",
    "        c_start = patch_c * patch_size\n",
    "        c_end = min((patch_c + 1) * patch_size, w)\n",
    "\n",
    "        # set these to black\n",
    "        occluded_image[r_start:r_end, c_start:c_end, :] = 0\n",
    "\n",
    "    return occluded_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B5. Evaluation & Helper functions\n",
    "Defined helper functions for generating captions with each model, Dataset Helper which loads images and the main evaluation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:41:32.919335Z",
     "iopub.status.busy": "2025-04-11T01:41:32.918737Z",
     "iopub.status.idle": "2025-04-11T01:41:32.926598Z",
     "shell.execute_reply": "2025-04-11T01:41:32.925887Z",
     "shell.execute_reply.started": "2025-04-11T01:41:32.919314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generate captions for test images(smolvlm)\n",
    "def generate_smolvlm_caption(image: Image.Image, processor, model, device, max_new_tokens):\n",
    "    \"\"\" Generates caption for a single image using SmolVLM. \"\"\"\n",
    "    if model is None or processor is None:\n",
    "        return \"Error: SmolVLM not loaded\"\n",
    "    try:\n",
    "        prompt = \"<image>\\nDescribe the image:\"\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False) # Use deterministic generation for consistency\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        caption = generated_text.split(\"Describe the image:\")[-1].strip()\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"Error during SmolVLM generation: {e}\"\n",
    "\n",
    "\n",
    "# generate captions for test images(custom model)\n",
    "def generate_custom_caption(pixel_values: torch.Tensor, model, tokenizer, device, max_length):\n",
    "    \"\"\" Generates caption for a single processed image using the custom model. \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "         return \"Error: Custom model not loaded\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": max_length,\n",
    "                \"num_beams\": 4,\n",
    "                \"pad_token_id\": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else model.model.config.pad_token_id,\n",
    "                \"decoder_start_token_id\": model.model.config.decoder_start_token_id\n",
    "            }\n",
    "            if pixel_values.dim() == 3:\n",
    "                pixel_values = pixel_values.unsqueeze(0)\n",
    "            pixel_values = pixel_values.to(device)\n",
    "\n",
    "            generated_ids = model.generate(pixel_values=pixel_values, gen_kwargs=gen_kwargs)\n",
    "        decoded_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "        return decoded_caption\n",
    "    except Exception as e:\n",
    "        return f\"Error during Custom Model generation: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:41:40.148221Z",
     "iopub.status.busy": "2025-04-11T01:41:40.147525Z",
     "iopub.status.idle": "2025-04-11T01:41:40.153562Z",
     "shell.execute_reply": "2025-04-11T01:41:40.152793Z",
     "shell.execute_reply.started": "2025-04-11T01:41:40.148197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageOcclusionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_filename = row['filename']\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "        try:\n",
    "            original_image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            original_image_pil = Image.new('RGB', (224, 224)) \n",
    "            img_filename = f\"ERROR_{img_filename}\"\n",
    "\n",
    "        return {\n",
    "            \"image_pil\": original_image_pil,\n",
    "            \"image_id\": img_filename,\n",
    "            \"original_caption\": row['caption']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:41:46.012298Z",
     "iopub.status.busy": "2025-04-11T01:41:46.011573Z",
     "iopub.status.idle": "2025-04-11T01:41:46.027725Z",
     "shell.execute_reply": "2025-04-11T01:41:46.027185Z",
     "shell.execute_reply.started": "2025-04-11T01:41:46.012274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_on_occluded_images(model, processor_or_tokenizer, image_processor, dataloader, device, occlusion_levels, model_key, gen_params, raw_results_output_dir):\n",
    "    \"\"\"\n",
    "    Evaluates a model on occluded images using a DataLoader. Saves raw results internally to CSVs. Returns scores dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    if model is None: print(f\"Model {model_key} is None. Skipping evaluation.\"); return None\n",
    "    print(f\"--- Starting Occlusion Evaluation for {model_key.upper()} (Will save raw CSVs) ---\")\n",
    "    levels_to_evaluate = occlusion_levels # Include baseline\n",
    "\n",
    "    raw_results_per_level = {level: {'preds': [], 'refs': [], 'ids': [], 'orig_captions': []} for level in levels_to_evaluate}\n",
    "\n",
    "    try:\n",
    "        bleu_metric = evaluate.load(\"bleu\"); rouge_metric = evaluate.load(\"rouge\"); meteor_metric = evaluate.load(\"meteor\")\n",
    "    except Exception as e: print(f\"Error loading metrics for {model_key}: {e}.\"); return None\n",
    "\n",
    "    model.eval(); torch.cuda.empty_cache() \n",
    "    with torch.no_grad():\n",
    "        # for all batches\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {model_key}\"):\n",
    "            batch_image_pils = batch[\"image_pil\"]; batch_image_ids = batch[\"image_id\"]; batch_original_captions = batch[\"original_caption\"]\n",
    "            # for each image\n",
    "            for i in range(len(batch_image_ids)): \n",
    "                image_pil, image_id, original_caption = batch_image_pils[i], batch_image_ids[i], batch_original_captions[i]\n",
    "                if \"ERROR_\" in image_id: continue\n",
    "                try: original_image_np = np.array(image_pil)\n",
    "                except Exception as e: print(f\"Error converting {image_id} to numpy: {e}. Skipping.\"); continue\n",
    "\n",
    "                for occ_level in levels_to_evaluate:\n",
    "                    occluded_image_np = occlude_image(original_image_np, occ_level, PATCH_SIZE)\n",
    "                    # convert back to image\n",
    "                    occluded_image_pil = Image.fromarray(occluded_image_np)\n",
    "                    pred_caption = f\"Error - Generation Failed\"\n",
    "                    try:\n",
    "                        if model_key == 'smolvlm':\n",
    "                            pred_caption = generate_smolvlm_caption(occluded_image_pil, processor_or_tokenizer, model, device, **gen_params)\n",
    "                        elif model_key == 'custom':\n",
    "                            if image_processor is None: raise ValueError(\"Image processor needed\")\n",
    "                            pixel_values = image_processor(images=occluded_image_pil, return_tensors=\"pt\").pixel_values\n",
    "                            pred_caption = generate_custom_caption(pixel_values, model, processor_or_tokenizer, device, **gen_params)\n",
    "                    except Exception as e: pred_caption = f\"Error during generation: {e}\"\n",
    "\n",
    "                    raw_results_per_level[occ_level]['preds'].append(pred_caption)\n",
    "                    raw_results_per_level[occ_level]['refs'].append([original_caption])\n",
    "                    raw_results_per_level[occ_level]['ids'].append(image_id)\n",
    "                    raw_results_per_level[occ_level]['orig_captions'].append(original_caption)\n",
    "\n",
    "\n",
    "    # save all raw results to CSV\n",
    "    print(f\"\\nSaving raw results CSV files for {model_key} to {raw_results_output_dir}...\")\n",
    "    for level in levels_to_evaluate:\n",
    "        level_int = int(level*100)\n",
    "        filename = os.path.join(raw_results_output_dir, f\"raw_{model_key}_{level_int}.csv\")\n",
    "        data_for_level = raw_results_per_level[level]\n",
    "        if data_for_level['ids']:\n",
    "             try:\n",
    "                 df_raw = pd.DataFrame({\n",
    "                     'image_id': data_for_level['ids'],\n",
    "                     'original_caption': data_for_level['orig_captions'],\n",
    "                     'generated_caption': data_for_level['preds']\n",
    "                 })\n",
    "                 df_raw.to_csv(filename, index=False, encoding='utf-8')\n",
    "                 print(f\"Saved {filename}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Error saving raw results to {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping save for {model_key} at {level*100}% - no data.\")\n",
    "\n",
    "\n",
    "    # calculate the 3 metrics\n",
    "    print(f\"\\nCalculating final metrics for {model_key}...\")\n",
    "    scores_per_level = {}\n",
    "    for level in levels_to_evaluate:\n",
    "        data = raw_results_per_level[level]; preds = data['preds']; refs = data['refs']; meteor_refs = data['orig_captions']\n",
    "        if not preds or not refs:\n",
    "            scores_per_level[level] = {'BLEU': 0, 'ROUGE-L': 0, 'METEOR': 0, 'Error': 'No data'}\n",
    "            continue\n",
    "        try:\n",
    "            preds = [str(p) if pd.notna(p) else \"\" for p in preds]; meteor_refs = [str(r) if pd.notna(r) else \"\" for r in meteor_refs]; refs = [[str(r[0]) if pd.notna(r[0]) else \"\"] for r in refs]\n",
    "            bleu_score = bleu_metric.compute(predictions=preds, references=refs)['bleu']; rouge_score = rouge_metric.compute(predictions=preds, references=refs)['rougeL']; meteor_score = meteor_metric.compute(predictions=preds, references=meteor_refs)['meteor']\n",
    "            scores_per_level[level] = { 'BLEU': bleu_score, 'ROUGE-L': rouge_score, 'METEOR': meteor_score }\n",
    "        except Exception as e: print(f\"Error calculating metrics for {model_key}@{level*100}%: {e}\"); scores_per_level[level] = {'BLEU': 0, 'ROUGE-L': 0, 'METEOR': 0, 'Error': str(e)}\n",
    "\n",
    "    print(f\"--- Finished Occlusion Evaluation for {model_key.upper()} ---\")\n",
    "    return scores_per_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:42:07.430338Z",
     "iopub.status.busy": "2025-04-11T01:42:07.429695Z",
     "iopub.status.idle": "2025-04-11T01:42:07.447857Z",
     "shell.execute_reply": "2025-04-11T01:42:07.447284Z",
     "shell.execute_reply.started": "2025-04-11T01:42:07.430317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoader with 116 batches.\n"
     ]
    }
   ],
   "source": [
    "# Create the test dataloader for our model etsting\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    test_dataset = ImageOcclusionDataset(test_df, TEST_IMAGES_PATH)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda batch: {key: [dic[key] for dic in batch] for key in batch[0]})\n",
    "    print(f\"Created DataLoader with {len(test_dataloader)} batches.\")\n",
    "except FileNotFoundError: print(f\"FATAL: Test CSV not found at {TEST_CSV_PATH}.\"); test_dataloader = None\n",
    "except Exception as e: print(f\"Error creating DataLoader: {e}\"); test_dataloader = None\n",
    "\n",
    "smolvlm_gen_params = {\"max_new_tokens\": 50}\n",
    "custom_gen_params = {\"max_length\": 50}\n",
    "\n",
    "smolvlm_final_scores = None\n",
    "custom_final_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on both smolVLM and our custom model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:43:34.695035Z",
     "iopub.status.busy": "2025-04-11T01:43:34.694768Z",
     "iopub.status.idle": "2025-04-11T06:06:18.565460Z",
     "shell.execute_reply": "2025-04-11T06:06:18.564789Z",
     "shell.execute_reply.started": "2025-04-11T01:43:34.695016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Occlusion Evaluation for SMOLVLM (Will save raw CSVs) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Evaluating smolvlm: 100%|██████████| 116/116 [4:22:16<00:00, 135.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving raw results CSV files for smolvlm to part_b_raw_results_csv...\n",
      "Saved part_b_raw_results_csv/raw_smolvlm_10.csv\n",
      "Saved part_b_raw_results_csv/raw_smolvlm_50.csv\n",
      "Saved part_b_raw_results_csv/raw_smolvlm_80.csv\n",
      "\n",
      "Calculating final metrics for smolvlm...\n",
      "--- Finished Occlusion Evaluation for SMOLVLM ---\n"
     ]
    }
   ],
   "source": [
    "if test_dataloader and smolvlm_model:\n",
    "    smolvlm_final_scores = evaluate_on_occluded_images(\n",
    "        model=smolvlm_model,\n",
    "        processor_or_tokenizer=smolvlm_processor,\n",
    "        image_processor=None, \n",
    "        dataloader=test_dataloader,\n",
    "        device=DEVICE,\n",
    "        occlusion_levels=OCCLUSION_LEVELS_ARG,\n",
    "        model_key='smolvlm',\n",
    "        gen_params=smolvlm_gen_params,\n",
    "        raw_results_output_dir=RAW_RESULTS_DIR \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:06:18.566774Z",
     "iopub.status.busy": "2025-04-11T06:06:18.566560Z",
     "iopub.status.idle": "2025-04-11T06:06:18.572131Z",
     "shell.execute_reply": "2025-04-11T06:06:18.571531Z",
     "shell.execute_reply.started": "2025-04-11T06:06:18.566759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scores returned by evaluate_on_occluded_images for Custom Model ---\n",
      "\n",
      "Occlusion Level: 10%\n",
      "  BLEU:    0.05252\n",
      "  ROUGE-L:  0.225322\n",
      "  METEOR:  0.26010\n",
      "\n",
      "Occlusion Level: 50%\n",
      "  BLEU:    0.03954\n",
      "  ROUGE-L:  0.176309\n",
      "  METEOR:  0.19322\n",
      "\n",
      "Occlusion Level: 80%\n",
      "  BLEU:    0.01424\n",
      "  ROUGE-L:  0.104451\n",
      "  METEOR:  0.10684\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Scores returned by evaluate_on_occluded_images for Custom Model ---\")\n",
    "if isinstance(smolvlm_final_scores, dict):\n",
    "        sorted_levels = sorted([float(k) for k in smolvlm_final_scores.keys()])\n",
    "\n",
    "        for level in sorted_levels:\n",
    "             level_key = float(level) \n",
    "             scores = smolvlm_final_scores.get(level_key)\n",
    "             print(f\"\\nOcclusion Level: {level_key*100:.0f}%\")\n",
    "             if isinstance(scores, dict):\n",
    "                 print(f\"  BLEU:    {scores.get('BLEU', 'N/A'):.5f}\")\n",
    "                 print(f\"  ROUGE-L: {scores.get('ROUGE-L', 'N/A'): 5f}\")\n",
    "                 print(f\"  METEOR:  {scores.get('METEOR', 'N/A'):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:06:18.572929Z",
     "iopub.status.busy": "2025-04-11T06:06:18.572727Z",
     "iopub.status.idle": "2025-04-11T06:40:11.282018Z",
     "shell.execute_reply": "2025-04-11T06:40:11.281389Z",
     "shell.execute_reply.started": "2025-04-11T06:06:18.572915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Occlusion Evaluation for CUSTOM (Will save raw CSVs) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Evaluating custom:   0%|          | 0/116 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Evaluating custom: 100%|██████████| 116/116 [33:37<00:00, 17.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving raw results CSV files for custom to part_b_raw_results_csv...\n",
      "Saved part_b_raw_results_csv/raw_custom_10.csv\n",
      "Saved part_b_raw_results_csv/raw_custom_50.csv\n",
      "Saved part_b_raw_results_csv/raw_custom_80.csv\n",
      "\n",
      "Calculating final metrics for custom...\n",
      "--- Finished Occlusion Evaluation for CUSTOM ---\n"
     ]
    }
   ],
   "source": [
    "if test_dataloader and custom_model:\n",
    "    custom_final_scores = evaluate_on_occluded_images(\n",
    "        model=custom_model,\n",
    "        processor_or_tokenizer=custom_tokenizer,\n",
    "        image_processor=custom_image_processor,\n",
    "        dataloader=test_dataloader,\n",
    "        device=DEVICE,\n",
    "        occlusion_levels=OCCLUSION_LEVELS_ARG,\n",
    "        model_key='custom',\n",
    "        gen_params=custom_gen_params,\n",
    "        raw_results_output_dir=RAW_RESULTS_DIR \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:40:11.284367Z",
     "iopub.status.busy": "2025-04-11T06:40:11.284014Z",
     "iopub.status.idle": "2025-04-11T06:40:11.289488Z",
     "shell.execute_reply": "2025-04-11T06:40:11.288894Z",
     "shell.execute_reply.started": "2025-04-11T06:40:11.284333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scores returned by evaluate_on_occluded_images for Custom Model ---\n",
      "\n",
      "Occlusion Level: 10%\n",
      "  BLEU:    0.06307\n",
      "  ROUGE-L:  0.283749\n",
      "  METEOR:  0.23333\n",
      "\n",
      "Occlusion Level: 50%\n",
      "  BLEU:    0.04353\n",
      "  ROUGE-L:  0.252464\n",
      "  METEOR:  0.20752\n",
      "\n",
      "Occlusion Level: 80%\n",
      "  BLEU:    0.03392\n",
      "  ROUGE-L:  0.240810\n",
      "  METEOR:  0.19795\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Scores returned by evaluate_on_occluded_images for Custom Model ---\")\n",
    "if isinstance(custom_final_scores, dict):\n",
    "        sorted_levels = sorted([float(k) for k in custom_final_scores.keys()])\n",
    "\n",
    "        for level in sorted_levels:\n",
    "             level_key = float(level) \n",
    "             scores = custom_final_scores.get(level_key)\n",
    "             print(f\"\\nOcclusion Level: {level_key*100:.0f}%\")\n",
    "             if isinstance(scores, dict):\n",
    "                 print(f\"  BLEU:    {scores.get('BLEU', 'N/A'):.5f}\")\n",
    "                 print(f\"  ROUGE-L: {scores.get('ROUGE-L', 'N/A'): 5f}\")\n",
    "                 print(f\"  METEOR:  {scores.get('METEOR', 'N/A'):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B8. Analysis and Saving data for next PART\n",
    "\n",
    "Uses the final scores returned by the evaluation functions for analysis. Loads the raw CSV files (saved internally by the functions) to assemble Part C data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:42:54.690571Z",
     "iopub.status.busy": "2025-04-11T06:42:54.689882Z",
     "iopub.status.idle": "2025-04-11T06:42:54.840534Z",
     "shell.execute_reply": "2025-04-11T06:42:54.839840Z",
     "shell.execute_reply.started": "2025-04-11T06:42:54.690550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregating Results and Saving Part C Data ---\n",
      "\n",
      "--- Final Performance Summary (Using Part A Baselines) ---\n",
      "\n",
      "Baseline (0% Occlusion - from Part A):\n",
      "  SmolVLM: {'BLEU': 0.05751116213026179, 'ROUGE-L': 0.23192061369592903, 'METEOR': 0.2687414556395594}\n",
      "  Custom : {'BLEU': 0.0696919985483392, 'ROUGE-L': 0.2914559915144349, 'METEOR': 0.23919343552885367}\n",
      "\n",
      "Occlusion Level: 10%\n",
      "  SmolVLM: {'BLEU': 0.05251569996354642, 'ROUGE-L': 0.2253222082555746, 'METEOR': 0.2601031894310235}\n",
      "  Custom : {'BLEU': 0.06306653453007512, 'ROUGE-L': 0.2837485057567668, 'METEOR': 0.23333117133535802}\n",
      "\n",
      "Occlusion Level: 50%\n",
      "  SmolVLM: {'BLEU': 0.03954059454316779, 'ROUGE-L': 0.17630869575191552, 'METEOR': 0.1932211553338966}\n",
      "  Custom : {'BLEU': 0.043532610279796144, 'ROUGE-L': 0.2524637112278998, 'METEOR': 0.20752224399128041}\n",
      "\n",
      "Occlusion Level: 80%\n",
      "  SmolVLM: {'BLEU': 0.014235201519826829, 'ROUGE-L': 0.10445087834211797, 'METEOR': 0.10684116586939828}\n",
      "  Custom : {'BLEU': 0.033921258602148105, 'ROUGE-L': 0.2408103505219092, 'METEOR': 0.19794785437117024}\n",
      "\n",
      "--- Performance Change (Score(Occluded) - Score(Baseline)) ---\n",
      "\n",
      "Occlusion Level: 10%\n",
      "  SmolVLM Change: BLEU=-0.0050, ROUGE-L=-0.0066, METEOR=-0.0086\n",
      "  Custom Change : BLEU=-0.0066, ROUGE-L=-0.0077, METEOR=-0.0059\n",
      "\n",
      "Occlusion Level: 50%\n",
      "  SmolVLM Change: BLEU=-0.0180, ROUGE-L=-0.0556, METEOR=-0.0755\n",
      "  Custom Change : BLEU=-0.0262, ROUGE-L=-0.0390, METEOR=-0.0317\n",
      "\n",
      "Occlusion Level: 80%\n",
      "  SmolVLM Change: BLEU=-0.0433, ROUGE-L=-0.1275, METEOR=-0.1619\n",
      "  Custom Change : BLEU=-0.0358, ROUGE-L=-0.0506, METEOR=-0.0412\n",
      "\n",
      "Loading raw data from part_b_raw_results_csv and assembling Part C data...\n",
      "Data for Part C saved successfully to final_raw_results.csv (2784 records).\n"
     ]
    }
   ],
   "source": [
    "baseline_smolvlm = {\n",
    "    'BLEU': 0.05751116213026179,\n",
    "    'ROUGE-L': 0.23192061369592903,\n",
    "    'METEOR': 0.2687414556395594\n",
    "}\n",
    "baseline_custom = {\n",
    "    'BLEU': 0.0696919985483392,\n",
    "    'ROUGE-L': 0.2914559915144349,\n",
    "    'METEOR': 0.23919343552885367\n",
    "}\n",
    "\n",
    "if smolvlm_final_scores is None or custom_final_scores is None:\n",
    "    print(\"\\nOne or both evaluations did not complete successfully or were skipped. Cannot aggregate results fully.\")\n",
    "else:\n",
    "    print(\"\\n--- Aggregating Results and Saving Part C Data ---\")\n",
    "\n",
    "    performance_summary = {'smolvlm': {}, 'custom': {}}\n",
    "    performance_summary['smolvlm'] = smolvlm_final_scores \n",
    "    performance_summary['custom'] = custom_final_scores \n",
    "\n",
    "    # using score results from part A\n",
    "    performance_summary['smolvlm'][0.0] = baseline_smolvlm\n",
    "    performance_summary['custom'][0.0] = baseline_custom\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Performance Summary (Using Part A Baselines) ---\")\n",
    "    print(f\"\\nBaseline (0% Occlusion - from Part A):\")\n",
    "    print(f\"  SmolVLM: {performance_summary['smolvlm'].get(0.0, 'N/A')}\")\n",
    "    print(f\"  Custom : {performance_summary['custom'].get(0.0, 'N/A')}\")\n",
    "\n",
    "    for level in OCCLUSION_LEVELS_ARG: \n",
    "        print(f\"\\nOcclusion Level: {int(level*100)}%\")\n",
    "        print(f\"  SmolVLM: {performance_summary['smolvlm'].get(level, 'N/A - Evaluation might have failed')}\")\n",
    "        print(f\"  Custom : {performance_summary['custom'].get(level, 'N/A - Evaluation might have failed')}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Performance Change (Score(Occluded) - Score(Baseline)) ---\")\n",
    "    valid_baseline_smol = isinstance(baseline_smolvlm, dict) and 'Error' not in baseline_smolvlm\n",
    "    valid_baseline_cust = isinstance(baseline_custom, dict) and 'Error' not in baseline_custom\n",
    "\n",
    "    # calculating changes for each occlusion level\n",
    "    for level in OCCLUSION_LEVELS_ARG: \n",
    "        print(f\"\\nOcclusion Level: {int(level*100)}%\")\n",
    "\n",
    "        # score for smolvlm \n",
    "        current_perf_smol = performance_summary['smolvlm'].get(level) \n",
    "        valid_current_smol = isinstance(current_perf_smol, dict) and 'Error' not in current_perf_smol\n",
    "        if valid_baseline_smol and valid_current_smol:\n",
    "             try:\n",
    "                 delta_bleu = current_perf_smol['BLEU'] - baseline_smolvlm['BLEU']\n",
    "                 delta_rouge = current_perf_smol['ROUGE-L'] - baseline_smolvlm['ROUGE-L']\n",
    "                 delta_meteor = current_perf_smol['METEOR'] - baseline_smolvlm['METEOR']\n",
    "                 print(f\"  SmolVLM Change: BLEU={delta_bleu:+.4f}, ROUGE-L={delta_rouge:+.4f}, METEOR={delta_meteor:+.4f}\")\n",
    "             except KeyError as ke: print(f\"  SmolVLM Change: Error calculating change - Missing key {ke}\")\n",
    "             except Exception as e: print(f\"  SmolVLM Change: Error calculating change - {e}\")\n",
    "        else: print(f\"  SmolVLM Change: Cannot calculate change due to missing/invalid data.\")\n",
    "\n",
    "        # score for custom model\n",
    "        current_perf_cust = performance_summary['custom'].get(level) \n",
    "        valid_current_cust = isinstance(current_perf_cust, dict) and 'Error' not in current_perf_cust\n",
    "        if valid_baseline_cust and valid_current_cust:\n",
    "             try:\n",
    "                 delta_bleu = current_perf_cust['BLEU'] - baseline_custom['BLEU']\n",
    "                 delta_rouge = current_perf_cust['ROUGE-L'] - baseline_custom['ROUGE-L']\n",
    "                 delta_meteor = current_perf_cust['METEOR'] - baseline_custom['METEOR']\n",
    "                 print(f\"  Custom Change : BLEU={delta_bleu:+.4f}, ROUGE-L={delta_rouge:+.4f}, METEOR={delta_meteor:+.4f}\")\n",
    "             except KeyError as ke: print(f\"  Custom Change : Error calculating change - Missing key {ke}\")\n",
    "             except Exception as e: print(f\"  Custom Change : Error calculating change - {e}\")\n",
    "        else: print(f\"  Custom Change : Cannot calculate change due to missing/invalid data.\")\n",
    "\n",
    "\n",
    "    # saving final data for part c\n",
    "    print(f\"\\nLoading raw data from {RAW_RESULTS_DIR} and assembling Part C data...\")\n",
    "    part_c_dfs = []\n",
    "    error_loading_raw = False\n",
    "    for level in OCCLUSION_LEVELS_ARG: \n",
    "        level_int = int(level*100)\n",
    "        smol_file = os.path.join(RAW_RESULTS_DIR, f\"raw_smolvlm_{level_int}.csv\")\n",
    "        cust_file = os.path.join(RAW_RESULTS_DIR, f\"raw_custom_{level_int}.csv\")\n",
    "        df_smol, df_cust = None, None\n",
    "\n",
    "        try: \n",
    "            if os.path.exists(smol_file): df_smol = pd.read_csv(smol_file)\n",
    "            else: print(f\"Warning: Raw file not found: {smol_file}\")\n",
    "            if os.path.exists(cust_file): df_cust = pd.read_csv(cust_file)\n",
    "            else: print(f\"Warning: Raw file not found: {cust_file}\")\n",
    "        except Exception as e: print(f\"Error loading raw CSVs for level {level_int}%: {e}\"); error_loading_raw = True; continue\n",
    "\n",
    "        if df_smol is None or df_cust is None:\n",
    "            print(f\"Warning: Missing raw data CSV for level {level_int}%. Cannot include in Part C data.\"); error_loading_raw = True; continue\n",
    "\n",
    "        # two df for each model\n",
    "        df_smol = df_smol.rename(columns={'generated_caption': 'generated_caption_smolvlm'})\n",
    "        df_cust = df_cust.rename(columns={'generated_caption': 'generated_caption_custom'})\n",
    "        merged_df = pd.merge(\n",
    "            df_smol[['image_id', 'original_caption', 'generated_caption_smolvlm']],\n",
    "            df_cust[['image_id', 'generated_caption_custom']], on='image_id', how='inner' \n",
    "        )\n",
    "        merged_df['perturbation_percentage'] = level_int\n",
    "        part_c_dfs.append(merged_df)\n",
    "\n",
    "    # merged both parts\n",
    "    if part_c_dfs:\n",
    "        final_part_c_df = pd.concat(part_c_dfs, ignore_index=True)\n",
    "        try:\n",
    "            final_part_c_df = final_part_c_df[['image_id', 'original_caption', 'generated_caption_smolvlm', 'generated_caption_custom', 'perturbation_percentage']]\n",
    "            final_part_c_df.to_csv(PART_C_DATA_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "            print(f\"Data for Part C saved successfully to {PART_C_DATA_OUTPUT_FILE} ({len(final_part_c_df)} records).\")\n",
    "        except Exception as e: print(f\"Error saving final Part C data to CSV: {e}\")\n",
    "    elif not error_loading_raw: print(\"No data assembled for Part C (check if raw CSV files exist in output dir). CSV file not saved.\")\n",
    "    else: print(\"Could not assemble Part C data due to errors loading raw CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
